---
title: "Meeting 7 - Probability Distributions and the Central Limit Theorem"
author: "Matthew E. Aiello-Lammens"
date: '2020-03-11'
output:
  html_document:
    code_folding: hide
    toc: yes
---

# Today's Goals

* Finish introduction to probability distributions
* Identify some important distributions used in biostatistics
* Use the knowledge of the normal distribution to understand the Central Limit Theorem (CLT)
* Learn the link between the standard normal distribution, the CLT, and Confidence Intervals

***

# Probability distributions (cont'd)

## Uncertainty in biological data

Fact: there is a lot of uncertainty associated with biological data.
Using probability distributions in our analysis actually helps us deal with some of this uncertainty.

There are two types of uncertainty:

* **Process uncertainty:** A result of our imperfect knowledge of things.
Example: we may get two different mean values for petal length for a particular Iris species if we go out to a field and measure two different sets of 50 flowers because we don't have perfect knowledge of all of the many factors that go into determining petal length.

* **Observation uncertainty:** Inaccuracies resulting during measurement. 
Example: our petal lengths values may be "off" if we used two different rulers, which were not exactly the same.

We will focus more on **Process uncertainty**, than on **Observation uncertainty** in this class.
**We try to understand uncertainty and uncertain outcomes by using probability distributions.**



## Histograms and density plots as probability distributions

Let's go back to the plots of petal length from a few classes ago. 
**Recall that when considering density plots the area under the curve or the area of the bins is equal to 1.**

```{r}
library(ggplot2)

ggplot() +
  geom_histogram(data = iris, 
                 aes(x = Petal.Length, y = ..density.., fill = Species)) +
  facet_grid( Species ~ . ) +
  geom_density(data = iris, aes(x = Petal.Length, colour = Species)) +
  theme_bw()  
```

**Key idea: We can think of the area for a particular bin as the probability of getting a value that falls into that bin.**


### Emperical distributions versus defined probability distributions

There are *many* defined probability distributions that have specific properties.
Last week we derived the Binomial Distribution and were introduced to the Bernoulli Distribution. 
Some properties of distributions to keep in mind:

* The area under the curve, or cumulative area of the bins is equal to 1
* Different values of the variable described by a distribution are on the x-axis
* The corresponding probability value for that particular variable value is on the y-axis (or expressed by the total area of the bin in a histogram plot). *Note - this is not the case for continuous distributions, which we'll discuss later.*
* We often want to focus on the probability distribution(s) that are related to our specific questions. Many of the most common distributions, though not all, can be interpreted as providing the answer to particular questions. Below I have outlined a number of distributions and the questions we might use them to answer.

## Generic Distribution:

A probability distribution is simply a way to describe the probability that some event occurs, given a set of rules (i.e., a function).
Mathematically we can write this as:

$$
P( a < x < b) = \int_a^b f(x|params)dx
$$

***

# Important Statistical Distributions

## Normal Distribution:

The normal distribution is perhaps the most widely used distribution in life science. 
It is also probably the most familiar.

The probability density function for the normal is:

$$
f(x|\mu,\sigma) ~ \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

Let's dive into working with the Normal in R.

In R, there are several things you can do with all kinds of distributions (not just the normal).
Start by looking for help on `rnorm`

```{r}
?rnorm
```

Note that there is `dnorm`, `pnorm`, `qnorm`, and `rnorm`. 
For now, know that `rnorm` generates random variables from the distribution.

```{r}
normal_rvs <- rnorm(n = 100, mean = 0, sd = 1)

ggplot() + 
  geom_histogram( data = NULL, aes(x = normal_rvs) ) +
  theme_bw()
```

#### Challenge

1. Increase the number of random variables drawn.
2. Change the `mean` and `sd` parameters.

## Standard Normal distribution

The **Standard Normal** distribution is a **Normal** distribution with $\mu = 0$ and $\sigma = 1$.

#### Challenge

Draw 1000 samples from the standard normal distribution and plot a histogram and density curve for the resulting data.
After making this plot, try 10,000 samples.

```{r}
stdnorm_samps <- data.frame( samps = rnorm(n = 10000, mean = 0, sd = 1) )

ggplot(data = stdnorm_samps, aes(x = samps)) +
  geom_histogram( aes( y = ..density.. ) ) +
  geom_density(colour = "red", size = 2) + 
  theme_bw()
```

* How well does the density plot represent the standard normal distribution?
* Can we add the functional form of the normal distribution?

```{r, fig.show='hide'}
ggplot(data = stdnorm_samps, aes(x = samps)) +
  geom_histogram( aes( y = ..density.. ) ) +
  geom_density(colour = "red", size = 2) + 
  stat_function( fun = dnorm, colour = "blue", alpha = 0.6, size = 2) +
  theme_bw()
```


We can convert from a normal distribution with any mean $\mu$ and standard deviation $\sigma$ to the standard normal by subtracting the mean from each value, then dividing by the standard deviation:

$$
z_i = \frac{y_i - \mu}{\sigma}
$$

### Z-scores

This same process can be used to calculate what are called **z-scores**. We can calculate z-scores for observations in *any* data set (though they are not always necessarily standard normally distributed).

## Negative Binomial Distribution:

The coin flip problem from [Problem 2 in Problem Set 2](http://mlammens.github.io/ENS-623-Research-Stats/problem_sets/Problem-set-2.html) was actually an example of simulating a process that is Negative Binomially distributed. 
This distribution is related to the Binomial distribution, but where as
for the Binomial distribution we set the number of trials, $N$, as fixed, and asked
about the number of successes, $x$, in the **Negative** binomial we 
set the number of successes as fixed, $x$, and ask 
**how many trials, $N$, does it takes to get that number of successes**.

$$
{pdf} = P(N|x,h) = \binom{N-1}{x-1} h^x (1-h)^{N-x}
$$

*Question:* What is the number of trials, $N$, needed to reach the
$x$ th success.

```{r}
head_count <- 0
tail_count <- 0

while(head_count <= 100){
  ifelse(test = runif(1) < 0.5, yes = (head_count <- head_count + 1), no = (tail_count <- tail_count + 1))
}

(flip_count <- head_count + tail_count)
```

### Geometric Distribution:

The Geometric distribution is a special case of the Negative
Binomial, where we are asking specifically about the number
of trials to reach the **first** success (i.e. the case of
the negative binomial where $x=1$.

$$
{pdf} = P(N|h) = h (1-h)^{N-1}
$$

*Question:* What is the number of trials, $N$, needed to reach the
1st success.

## Poisson Distribution:

There are many questions we may ask that are related
to the Poisson distribution. Usually we think of the 
Poisson when we have some process that usually results
in a small number most of the times and produces larger 
numbers less frequently. Think about the number of eggs produced
by some bird, or the number of off spring for some animal. 
By substituting the usual $\lambda$ value in the Poisson 
with $\lambda T$, where $T$ is some defined time period, and 
$\lambda$ is some rate value, **we can use the Poisson to address 
questions concerning the number of successes in some time 
period T**.

$$
{pdf} = P(x|T,\lambda) = \frac{ e^{-(\lambda T)} (\lambda T)^x }{ x! }
$$

## Other important distributions in data analysis we'll see this semester

* Student's $t$ distribution
* $\chi^2$ distribution
* $F$ distribution

***

### The utility of knowing the amount of area under the curve for probability densities

* Where do 95% of the values from the standard normal fall? **Introducing `qnorm`.**

```{r}
qnorm(.025)
qnorm(.975)
```

* How might we visualize this?

```{r}
ggplot(data = stdnorm_samps, aes(x = samps)) +
  coord_cartesian(xlim = c(-5, 5), ylim = c(0, 0.41)) +
  geom_vline(xintercept = c(-1.96, 1.96), size = 2 ) +
  stat_function( fun = dnorm, colour = "blue", size = 2) +
  theme_bw()
```


# The Central Limit Theorem

This section gets into more nitty-gritty probability and statistics. 
You should review this information in both of the course text books.

## Sampling distribution of the mean

The frequency distribution of the **mean values** of several samples taken from a population.

From Q&K p. 18:

> The probability distribution of means of samples from a normal distribution is also normally distributed.

#### Challenge

1. Take 100 samples of 10 observations from a normal distribution with a mean of 0 and a standard deviation of 1. Plot a histogram of the means.
2. Increase the number of samples. How does this affect the outcome?
3. Increase the number of observations. How does this affect the outcome?

```{r}


```


## CLT

In the limit as $n \to \infty$, the probability distribution of means of samples from ***any distribution*** will approach a normal distribution.

### Example with Poisson distribution

Plot a histogram of 100 observations that are **Poisson distributed** with a $\lambda$ value $= 1$.
Describe the shape of the distribution. 
Does it look normal?

```{r, fig.show='hide'}
ggplot(data = NULL) +
  geom_histogram(aes(x = rpois(100, lambda = 1)))
```


Next, take 100 samples of 100 observations from a **Poisson distribution**, calculate the mean for each sample. Plot a histogram of the means.

```{r}
pois_means <- c()

for(x in 1:100){
  new_pois_mean <- mean(rpois(100, lambda = 1))
  pois_means <- c(pois_means, new_pois_mean)
}

ggplot(data = NULL) +
  geom_histogram(aes(x = pois_means))

```

## Using the CLT

From Q&K p. 18:

> The expected value or mean of the probability distribution of sample means equals the mean of the population ($\mu$) from which the samples were taken.


### Standard error of the sample mean

We just demonstrated that **sample means are normally distributed**. 
What can we do with this?
To start, we can say something about the precision and accuracy of our estimates of the *population* mean based on the *sample* mean.

**Standard error of the mean** - the expected value of the standard deviation of the sample mean:

$$
\sigma_{\bar{y}} = \frac{\sigma}{\sqrt{n}}
$$

Often times we only have **one sample** from the population. What can we say then?

Standard error of the mean:

$$
s_{\bar{y}} = \frac{s}{\sqrt{n}}
$$

where $s$ is the sample estimate of the standard deviation of the original population and $n$ is the sample size.

**NB:** The standard error of the means "tells us about the error in using $\bar{y}$ to estimate $\mu$."


## Confidence intervals for the population mean

Recall that we can convert any observation from normal distribution to it's equivalent value from a standard normal distribution using:

$$
z_i = \frac{y_i - \mu}{\sigma}
$$

and we call these individual values ***z-scores***.

Using this formula, we can convert a sample mean, $\bar{y}$, into its corresponding value from a standard normal using:

$$
z = \frac{\bar{y} - \mu}{\sigma_{\bar{y}}}
$$

where the denominator is the standard error of the mean (see above). 

We can use this information to estimate how confident we are that are sample mean is a good representation of the *true* population mean.
Again, we are doing this by taking advantage of the fact that we know the sample means are *normally distributed*.
We calculate the range in which 95% of the sample mean values fall.

In mathematical terms, this looks like:

$$
P\{\bar{y} - 1.96\sigma_{\bar{y}} \leq \mu \leq \bar{y} + 1.96\sigma_{\bar{y}} \} = 0.95
$$

**VERY IMPORTANT:** The probability statement here is about the *interval*, not $\mu$. 
$\mu$ is not a random variable; it is fixed.
What we are saying is that there is 95% confidence that this interval includes the population mean, $\mu$.



### Unknown $\sigma$ (and $\sigma_{\bar{y}}$)

We rarely know $\sigma$, so we cannot calculate $\sigma_{\bar{y}}$. 
So what *can* we do?

Instead, we use the sample standard deviation, $s_{\bar{y}}$. 
So now we are dealing with a random variable that looks like: 

$$
\frac{\bar{y}-\mu}{s_{\bar{y}}}
$$

which is no longer standard normal distributed, but rather ***t* distributed**.

#### Challenge

Use whatever resource available to you, and look-up the ***t* distribution**.
What is/are the parameter(s) of the *t* distribution?


### Degrees of freedom

> The number of observations in our sample that are "free to vary" when we are estimating the variance (Q&K p.20; Box 2.1).

If I have calculated the mean, how many observations, out of a total of *n*, are free to vary?

**Answer:** $n-1$. Once I have the mean, I can use it and any $n-1$ values to calculate the *n*th value.

General rule regarding df's:

> the df is the number of observations minus the number of parameters included in the formula for the variance (Q&K p.20; Box 2.1).


## Standard error of other statistics

> The standard error is ... the standard deviation of the probability distribution of a specific statistics (Q&K p. 20).

### Standard error of the sample variance

The distribution of the sample variance is *not normal*. It is $\chi^2$ distributed.

Mathematically, this is explained as:

$$
X \thicksim N(0,1) \to X^2 \thicksim \chi^2
$$

Intuitively, why does this make sense?

***


# Important properties of distributions

## Measures of location, dispersion, and variability 

When we are looking at the distribution of any data set, we want to have a measure of the center of the distribution.
Usually our first step will be to look at the **arithmetic mean**, but each statistical distribution has its own **Expected Value**.

Similarly, we want to have a measure of dispersion and variability. 
Again, we usually would do something like calculate the **standard deviation**, but each distribution has its own measure of **Variance**.


### Terms and properties of distributions

Consider a generic probability distribution, with a PDF of 

$$
P( a < x < b) = \int_a^b f(x|params)dx
$$

(or an analogous PMF). This distribution has a number of properties that could be described.

* Parameters
    + Location - where
    + Shape - eponymous
    + Scale - spread
* Support - what values of $x$ are possible
* PDF or PMF
* Mean - expected value
* Median - middle value
* Mode - most common value
* Variance
* Skewness - measure of asymmetry
* Kurtosis - how fat/thin the tails are


## Estimating distribution parameters
 
**What are we estimating?**

Usually measures of **location** and **dispersion and variability**.

#### Challenge

Can you think of any measures of location and dispersion that you are familiar with?

## Expected value of a distribution

* Long-run average value
* Mean

### Expected value for discrete distributions

$$
E[X] = \sum_{i=1}^{\infty} x_i p_i
$$

### Expected value for continuous distributions

$$
E[X] = \int_{-\infty}^{\infty} x \cdot f(x) dx
$$

## Population versus sample statistics

We use **sample statistics** to estimate **population statistics**.
In most cases in biology, populations are too large to measure population parameters directly.
Therefore, we use different **estimators** to calculate the **populations statistics** based on the **sample statistics**.


### Properities of good estimators

See Q&K page 15 for more details.

1. Unbiased - repeated estimates should not consistently under- or over-estimate population parameters
2. Consistent - as sample size increases, sample estimate should get closer to population estimate
3. Efficient - estimate has lowest variance among other estimators

### Different kinds of estimators

* **Point estimate** - a single value estimate for a population parameter
* **Interval estimate** - a range of values that might include the parameter with a known probability

***

