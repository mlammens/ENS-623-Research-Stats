---
title: "Meeting 4 - Research Questions and Introduction to Probability"
author: "Matthew E. Aiello-Lammens"
date: '2020-02-19'
output:
  html_document:
    code_folding: hide
    toc: yes
---

# Today's Goals

* Apply R programming skills to simulate the process of flipping a coin until you get 100 heads
* Review the process of developing research questions
* Introduce some basic principles of probability
* Learn to simulate some basic probabilistic events


***

# Asking research questions


***

# Probability

> *We usually talk about probabilities in terms of events; the probability of event A occurring is written P(A). Probabilities can be between zero and one; if P(A) equals zero, then the event is impossible; if P(A) equals one, then the event is certain.* - Quinn and Keough 2002

## Frequentist statistics 

The relative frequency of an event over the long term, after many trials.

E.g., throwing a coin. What's the probability of a 'heads'? After one trial, two trials, ..., 1000 trials.

```{r}
(head <- runif(1) > 0.5)
```


## Bayesian statistics

For now, all I will say is that there are other perspectives in statistics. 
A key difference between Frequentist and Bayesian statistics is that the latter allows for you to incorporate **prior knowledge**.

### Flipping a coin - frequentist vs bayesian perspective

You find a coin on the ground, and want to know the probability that you will get a heads if you flip the coin. 
A frequentist approach would flip the coin many times and assume the probability of getting a heads is approximately the frequency of heads.
A Bayesian approach would assume that the probability is 0.5 based on past experience, then collect data (i.e., flip the coin) and update the *prior* probability using the newly acquired data.

***


# Properties of probability

* a **random process** gives rise to an **outcome**
    * a coin-flip is a random process
* 0 $\le$ P(*some event*) $\le$ 1
* $\hat{p}_n$ is the proportion of outcomes out of the total number of **trials** run
    * If we flip a coin 100 times, and 52 times we got heads, $\hat{p}_{100} = \frac{52}{100}$

## Law of Large Numbers

> *As more observations are collected, the proportion of $\hat{p}_n$ of occurrences with a particular outcome converges to the probability of $p$ of that outcome.* - Diez et al. 2016, p. 77

## Example of law of large numbers

Let's simulate a coin flip. To do this, we'll draw a single uniform random number between 0 and 1.

```{r}
(flip <- runif(1))
```

Let's define all values between 0 and 0.5 as being a "heads".
In order to *define* the flip as heads or tails, we'll need to use an `if` statement.

```{r}
if(flip < 0.5){
  coin_flip <- "head"
} else {
  coin_flip <- "tail"
}
```

Let's simulate the coin flip 100 times, and count the number of heads we get.

```{r}
# start a vector to store all of the flips
all_flips <- c()

for(x in 1:100){
  # "flip" the coin
  flip <- runif(1)
  
  # Identify the flip as heads or tails
  if(flip < 0.5){
    coin_flip <- "head"
  } else {
    coin_flip <- "tail"
  }
  
  # add this flip to the vector of all_flips
  all_flips <- c(all_flips, coin_flip)
  
}


```


Now, let's count the number of heads.

```{r}
all_flips == "head"
```

```{r}
sum(all_flips == "head")
```

Calculate the probability of a heads

```{r}
sum(all_flips == "head") / length(all_flips)
```

How close to 0.5 is this?

#### Challenge

Repeat the steps above until you get to a number of flips to make that consistently results in values *very* close to 0.5.

# Probability Distributions (Intro)

Let's look at a historgram of the total number of heads each of us got during the above experiment. 
From this figure, we can discuss the binomial probability distribution.

<!---
Go to this link [https://goo.gl/TeZvfT](https://goo.gl/TeZvfT) and add the number of heads that you got when we simulated 100 coin flips. 
If you don't remember, simply run the simulation again.

```{r, eval=FALSE}
library(googlesheets)
for_gs <- gs_title("Heads_Count")
heads_cnt <- gs_read(for_gs)
```

Make a histogram with these data.

```{r, eval=FALSE}
hist(heads_cnt$n_heads)
```
--->


***

# R programming recap

## Challenge: Number of coin flips it takes to get to 100 heads

Imagine you are flipping a fair coin (i.e., there is a 0.5 chance of getting a heads and a 0.5 chance of getting a tails).
You want to continue flipping the coin *until* you get 100 heads.
**Write a script that simulates coin flips, and counts the number of times you need to flip the coin until you get 100 heads.**
Note that since the coin flip is a random process, it will not necessarily take you the same number of flips if you re-run the script.

### Pseudo code

As we started last meeting, you might want to begin by writing some pseudo code

1. Begin by writing down the main things you need to do

```
- Flip a coin
- Determine if the coin is "heads" or "tails"
- Repeat the flip until you get 100 heads
- Tally the number of heads and the number of total flips
```

2. Start putting these things in order

3. Add in key components of the flow-control



*** 
















# Probability through M&Ms

Open your bag(s) and count how many M&Ms you got. Also count how many of each color.

Make a new variable called `tot_mm` and assign it the number of MMs in your bag.

```{r}
tot_mm <- 60
```

## Average number of MMs in a bag

Calculate the mean number of MMs in a bag.

```{r}
tot_mm_pop_sample <- c(60, 58, 57, 63, 61, 60, 59)
```

Mean:

$$
\bar{x} = \frac{\sum_{i=1}^n{x_i}}{n}
$$

Using the `sum` function.

```{r}
(mean_mm_1 <- sum(tot_mm_pop_sample) / length(tot_mm_pop_sample))
```

Using the `mean` function.

```{r}
(mean_mm_2 <- mean(tot_mm_pop_sample))
```

## Quantify the variation in the number of MMs in a bag

* `var` function
* `sd` function

**What do we mean by variation?**

In general, we mean something along the lines of 'the amount of variation around some mean value'.

**How might we quantify this?**

* Add up the distances from the mean. 

First calculate distances from the mean for each bag.

```{r}
(dist_from_mean <- tot_mm_pop_sample - mean_mm_1)
```

Next calculate the total distances. We could also call this the **sums of distances**

```{r}
(sum_dist_from_mean <- sum(dist_from_mean))
```

What information does this give us, and why might it not be useful?

* Add up the *absolute* distances form the mean. 

First calculate the *absolute* distances from the mean.

```{r}
(absdist_from_mean <- abs(tot_mm_pop_sample - mean_mm_1))
```

Next add up the total *absolute* distances from the mean.

```{r}
(sum_absdist_from_mean <- sum(absdist_from_mean))
```

* Add up the *squared* distances from the mean.

While the absolute distance from the mean does give us a reasonable measure of variability, because of specific mathematical properties, it's more convenient to work with *squared* distances from the mean, leading to the measures of **variance** and **standard deviation**.

#### Challenge

Calculate *squared* distances from the mean, and sum them to determine the total squared deviation.

**Standard deviation** can be thought of as the average deviation from the mean.

#### Challenge

1. Calculate the **variance** of the number of M&Ms in a bag, considering all of the bags in the class. Do this without using the `var` function.

2. Calculate the **standard deviation** of the number of M&Ms in a bag. Do this without using the `sd` function.

## Probability and M&Ms

Without looking, you chose one M&M. What colors could you have chosen?

The **set** of brown, yellow, green, red, orange, blue is the **sample space**.

Your selection of a single M&M is called an **event**.


#### Challenge

* What's the probability of getting a "color" M&M in **your** bag?

P(brown), P(yellow), etc.

### Putting our data together into a `data.frame`

Here we will collate the classes data into a single data set.

#### Challenge

What is the mean probability of getting each color across each bag?

Tools to use:

* `data.frames`
* `apply`


## Probability of events

* What's the P(green OR blue OR red) in **your** bag? 

P(green) + P(blue) + P(red)

* What is the P(NOT green) in **your** bag?

$$
P(\sim Green) = 1 - P(Green) = P(G)^c
$$


### Sampling with replacement

I draw one M&M from my bag, put it back, then draw another.

* What's the P(green and then blue)?

<!---
P(green) * P(blue) 
--->

* What's the P(green or blue)?

<!---
P(green) + P(blue)
--->

### Sampling without replacement

* What is the probability of getting green, then blue, *without replacing your first draw*?

#### Challenge

What is the sample space when drawing two M&Ms?


## Draw a random bag of M&Ms using the company stated frequencies/probabilities for each color

#### Challenge

How would you create a random bag of M&Ms, assuming that each M&M has an equal probability of being in any given bag?



Mars claims that percentages of each color M&Ms are slightly different.

```{r}
## Colors as a vector
mm_colors <- c("blue", "brown", "green", "orange", "red", "yellow")
mm_probs <- c(.23, .14, .16, .20, .13, .14)

## I want to "sample" a bag of MMs
new_bag <- sample(x = mm_colors, size = 15, replace = TRUE, prob = mm_probs)
table(new_bag)

```


Because it is possible to draw a bag that is completely missing some of the colors, we need to explicitly check how many of each color is in the new bag, if we want to compare with our original bag.


```{r}
## Count the number of each color
new_bag_counts <- c(sum(new_bag == "blue"),
                    sum(new_bag == "brown"),
                    sum(new_bag == "green"),
                    sum(new_bag == "orange"),
                    sum(new_bag == "red"),
                    sum(new_bag == "yellow"))
new_bag_counts
```

And now to check if my bag is the same as the new bag I can look at a series of logical tests.

```{r, eval=FALSE}
individual_color_compare <- new_bag_counts == my_orig_bag

# The bags match if all of these are true
all(individual_color_compare)
```


